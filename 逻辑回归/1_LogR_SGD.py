'''æ¢¯åº¦ä¸‹é™ç®—æ³•å¥½åƒä¸æ˜¯å¾ˆé€‚åˆç”¨åœ¨é€»è¾‘å›å½’çš„æ±‚è§£é‡Œ
æ— è®ºXï¼Œyï¼Œthetaæ˜¯é‡‡ç”¨matrixè¿˜æ˜¯ndarrayçš„å½¢å¼éƒ½å‡ºç°ä¸‹é¢çš„ç°è±¡ï¼Œåœ¨æœ¬æ–‡ä»¶ä¸­ï¼ŒXï¼Œyï¼Œthetaé‡‡ç”¨matrix
ä¸ºå•¥ä¼šå‡ºç°ä¸‹é¢è¿™äº›æƒ…å†µå‘¢ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿä¸æ‡‚å•Š
ç°è±¡1:æ‰§è¡Œa=y*np.log(hypothesis) + (1-y)*np.log(1-hypothesis)æ—¶æŠ¥RuntimeWarning
é”™è¯¯åŒ…æ‹¬divide by zero encountered in log, invalid value encountered in multiply
ç°è±¡2:allcosté‡Œè¿­ä»£è®°å½•æ¯3æ¬¡å‡ºç°ä¸€æ¬¡nanï¼ˆnot a numberï¼‰
ç°è±¡3:æ‰€å¾—è§£ä¸é«˜çº§ä¼˜åŒ–ç®—æ³•ç›¸å·®å¤ªå¤šï¼Œæœ€ç»ˆç”»å†³ç­–è¾¹ç•Œå›¾çš„æ—¶å€™å‘ç°æ‰€æ±‚å¾—çš„å†³ç­–è¾¹ç•Œå®Œå…¨æ²¡æœ‰èµ·åˆ°åˆ†ç±»çš„æ•ˆæœ
'''

# coding: utf-8
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data=pd.read_csv('/Users/zhangying/Desktop/243-ML-Ng/ex2-logistic regression/ex2data1.txt',header=None,names=['Exam1','Exam2','Class'])

positive=data[ data.Class.isin(['1']) ]
negative=data[ data.Class.isin(['0']) ]

def normalize_feature(df):
    return df.apply(lambda column : (column-column.mean())/column.std())
data=normalize_feature(data)    
#æˆ–è€…é‡‡ç”¨ğŸ‘‡ä¹Ÿå¯ä»¥ç‰¹å¾å½’ä¸€åŒ–
#data=(data-data.mean())/data.std() #ç‰¹å¾å½’ä¸€åŒ–

data.insert(0,'Ones',1)
X=data.iloc[:,0:3]
y=data.iloc[:,3:4] 

X=np.matrix(X.values) # (100,3) 
y=np.matrix(y.values) #  (100,1)  
theta=np.matrix(np.zeros(3))   # (1,3)

# print (X.shape, y.shape, theta.shape)

def sigmoid(theta,X):
    z=X*theta.T 
    return 1/(1+np.exp(-z))

originalhypothesis=sigmoid(theta,X) #matrixï¼Œshapeä¸ºï¼ˆ100,1ï¼‰
# print (type(originalhypothesis),originalhypothesis.shape)

def cost(theta,X,y):
    hypothesis=sigmoid(theta,X)
    a=np.multiply(y,np.log(hypothesis)) + np.multiply((1-y),np.log(1-hypothesis)) 
    return (-1./len(X))*np.sum(a)  

originalcost= cost(theta,X,y) #0.69314718056
print (originalcost)


def gradientdescent(X,y,theta,alpha,epochs):
    temp=np.matrix(np.ones(3)) #(1,3)
    allcost=np.zeros(epochs)
    for i in range(epochs):
        error=sigmoid(theta,X)-y #matrix (100,1)  
        temp=theta-(alpha/len(X))* error.T*X
        theta=temp
        allcost[i]=cost(theta,X,y) 
    return theta,allcost

alpha=0.1
epochs=200000
finaltheta,allcost=gradientdescent(X,y,theta,alpha,epochs)
#finaltheta [[-0.76754116  0.35868899 -0.11012487]]
#allcost[-1]ä¸º4.76424858314 ç­‰ä»·äºprint cost(X,y,finaltheta)
print (finaltheta)
print (allcost)

'''

def predict(theta,X):
    probability=sigmoid(theta,X) 
    return [1 if x>=0.5 else 0 for x in probability]

predictions=predict(finaltheta,X)
correct=[1 if a==b else 0 for (a,b) in zip(predictions,y)]
accuracy=sum(correct)/len(X)
#print (accuracy) #0.6

#ä¹Ÿå¯ä»¥ç”¨skearnä¸­çš„æ–¹æ³•æ¥æ£€éªŒé¢„æµ‹ç²¾åº¦ï¼Œå®Œå…¨ä¸çŸ¥é“ä¸ºå•¥å®ƒæµ‹å‡ºæ¥çš„ç²¾åº¦æ€»æ˜¯å¥‡é«˜æ— æ¯”ï¼Œæ— è¯­äº†
# from sklearn.metrics import classification_report
# print (classification_report(predictions,y)) 


x1=np.arange(130,step=0.1)
x2=-(finaltheta[0,0]+finaltheta[0,1]*x1)/finaltheta[0,2]


fig,ax=plt.subplots(figsize=(8,5))
ax.scatter(positive.Exam1,positive['Exam2'],color='black',marker='+',label='Admitted')
ax.scatter(negative['Exam1'],negative.Exam2,c='y',marker='o',label='Not admitted')
ax.plot(x1,x2)
ax.set_xlim(20,110)
ax.set_ylim(20,110)
ax.set_xlabel('Exam1 Score')
ax.set_ylabel('Exam2 Score')
ax.set_title('Decision Boundary')
ax.legend(loc=1) #å›¾ä¾‹æ˜¾ç¤ºåœ¨å³ä¸Šè§’
plt.show()
'''