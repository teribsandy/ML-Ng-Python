# coding: utf-8

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data=pd.read_csv('/Users/zhangying/Desktop/243-ML-å´æ©è¾¾/ex2-logistic regression/ex2data1.txt',header=None,names=['Exam1','Exam2','Class'])
#print data.head() #dataæ˜¯DataFrame #data.Classæ˜¯Series

#data.Class.isin(['1'])æ˜¯Series,å¸ƒå°”å€¼False True (Class=1åˆ™ä¸ºTrueï¼ŒClass=0åˆ™ä¸ºFalse)
positive=data[ data.Class.isin(['1']) ] #positiveæ˜¯DataFrameï¼Œåªæ˜¯æŠŠClasså…¨ä¸º1çš„æ•°æ®å–å‡ºæ¥äº†ï¼Œè¡Œç´¢å¼•æ¥è‡ªäºdata
#print positive.head()
negative=data[ data.Class.isin(['0']) ]

# fig,ax=plt.subplots(figsize=(8,5))
# ax.scatter(positive.Exam1,positive['Exam2'],color='black',marker='+',label='Admitted')
# ax.scatter(negative['Exam1'],negative.Exam2,c='y',marker='o',label='Not admitted')
# ax.set_xlabel('Exam1 Score')
# ax.set_ylabel('Exam2 Score')
# ax.set_title('scatter_binaryclassification')
# ax.legend(loc=1) #å›¾ä¾‹æ˜¾ç¤ºåœ¨å³ä¸Šè§’
# plt.show()
def normalize_feature(df):
    return df.apply(lambda column : (column-column.mean())/column.std())
data=normalize_feature(data)    
#æˆ–è€…é‡‡ç”¨ğŸ‘‡ä¹Ÿå¯ä»¥ç‰¹å¾å½’ä¸€åŒ–
#data=(data-data.mean())/data.std() #ç‰¹å¾å½’ä¸€åŒ–

data.insert(0,'Ones',1)
X=data.iloc[:,0:3]
y=data.iloc[:,3] #è‹¥y=data.iloc[:,3:4],åˆ™y=np.array(y.values)ç±»å‹è™½ç„¶ä¹Ÿæ˜¯ndarrayï¼Œä½†å½¢çŠ¶æ˜¯(100,1),åœ¨costå‡½æ•°è®¡ç®—æ—¶ä¼šæœ‰é—®é¢˜

X=np.array(X.values) #ndarray, (100,3) 
y=np.array(y.values) #ndarray, (100,)  

theta=np.zeros(3) #ndarray, (3,)


def sigmoid(theta,X):
    z=X@theta
    return 1/(1+np.exp(-z))

originalhypothesis=sigmoid(theta,X) #ndarrayï¼Œshapeä¸ºï¼ˆ100,ï¼‰ï¼Œåˆå§‹å€¼ä¸­å…ƒç´ å€¼å‡ä¸º0.5
# print (type(originalhypothesis),originalhypothesis.shape)

def cost(theta,X,y):
    hypothesis=sigmoid(theta,X)
    a=y*np.log(hypothesis) + (1-y)*np.log(1-hypothesis) #ndarray (100,) è®¡ç®—åˆå§‹æˆæœ¬ä¸‹ï¼Œaä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯-0.6931
    return (-1./len(X))*np.sum(a)  

originalcost= cost(theta,X,y) #0.69314718056, <class 'numpy.float64'> () 
print (originalcost)
# print (type(originalcost),originalcost.shape)


def gradient(theta,X,y):
    error=sigmoid(theta,X)-y #(100,)
    return (1.0/len(X))*X.T@error #(3,)
#print (gradient(theta,X,y)) #[ -0.1  -12.00921659  -11.26284221]

import scipy.optimize as opt

res=opt.minimize(fun=cost,x0=theta,args=(X,y),method='TNC',jac=gradient)
ftheta=res.x
print (res)
#fun: 0.20349770158947456 å³æœ€å°æˆæœ¬
#jac: array([9.12848998e-09, 9.69677383e-08, 4.84144623e-07]) å³æ±‚å¾—finalthetaå‰çš„åå¯¼
# x: array([-25.16131865,   0.20623159,   0.20147149]) å³finaltheta

'''
def predict(theta,X):
    probability=sigmoid(theta,X) 
    return [1 if x>=0.5 else 0 for x in probability]

predictions=predict(ftheta,X)
correct=[1 if a==b else 0 for (a,b) in zip(predictions,y)]
accuracy=sum(correct)/len(X)
#print (accuracy) #0.89
'''

'''ä¹Ÿå¯ä»¥ç”¨skearnä¸­çš„æ–¹æ³•æ¥æ£€éªŒé¢„æµ‹ç²¾åº¦
from sklearn.metrics import classification_report
print (classification_report(y,predictions))
'''

'''
x1=np.arange(130,step=0.1)
x2=-(ftheta[0]+ftheta[1]*x1)/ftheta[2]

fig,ax=plt.subplots(figsize=(8,5))
ax.scatter(positive.Exam1,positive['Exam2'],color='black',marker='+',label='Admitted')
ax.scatter(negative['Exam1'],negative.Exam2,c='y',marker='o',label='Not admitted')
ax.plot(x1,x2)
ax.set_xlim(20,110)
ax.set_ylim(20,110)
ax.set_xlabel('Exam1 Score')
ax.set_ylabel('Exam2 Score')
ax.set_title('Decision Boundary')
ax.legend(loc=1) #å›¾ä¾‹æ˜¾ç¤ºåœ¨å³ä¸Šè§’
plt.show()

'''