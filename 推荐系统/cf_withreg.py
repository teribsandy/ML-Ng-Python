#coding:utf-8

import numpy as np
from scipy import io

data=io.loadmat('/Users/zhangying/Desktop/243-ML-Ng/ex8-anomaly detection and recommendation/data/ex8_movies.mat')
Y,R=data['Y'],data['R'] #(1682,943) (1682,943)

# print (np.sum(R)) #100000 å…±æœ‰10wæ¡è¯„åˆ†è®°å½•ï¼Œä¸è¿‡æ¯”èµ·æ¥æ€»æ•°158wå¤šæ¥å¯ä»¥çŸ¥é“ç”¨æˆ·ç”µå½±è¯„åˆ†çŸ©é˜µæœ‰å¾ˆå¤šç©ºç™½å¤„ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯åˆ©ç”¨ä½ç§©é‡æ„è¿›è¡ŒçŸ©é˜µå¡«å……

params=io.loadmat('/Users/zhangying/Desktop/243-ML-Ng/ex8-anomaly detection and recommendation/data/ex8_movieParams.mat')
X,Theta=params['X'],params['Theta']
# (1682, 10) (943, 10)


def data(X,Theta,Y,R,nm=1682,nu=943,nf=10):
    X=X[:nm,:nf]
    Theta=Theta[:nu,:nf]
    Y=Y[:nm,:nu]
    R=R[:nm,:nu]
    return X,Theta,Y,R


def params_vector(X,Theta):
    return np.r_[X.ravel(),Theta.ravel()]


def params_seperate(XTvector,nm=1682,nu=943,nf=10):
    X=XTvector[:nm*nf].reshape(nm,nf)
    Theta=XTvector[nm*nf:].reshape(nu,nf)
    return X,Theta


def Cost(XTvector,Y,R,nm=1682,nu=943,nf=10,l=0):
    X,Theta=params_seperate(XTvector,nm,nu,nf)

    # ratedindex=np.where(R.ravel()==1)[0]
    # y=Y.ravel()[ratedindex]
    # h=(X@Theta.T).ravel()[ratedindex]
    # cost=.5*np.sum((h-y)**2)
    # reg=(l/2.)*(np.sum(X**2)+np.sum(Theta**2))
    

    #è™½ç„¶ç»“æœæ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯ä»ç®€ä¾¿æ€§ä¸Šæ¥è¯´ï¼Œå¼ºæ¨ğŸ‘‡è¿™ç§å“‡ï¼Œè°è®©Rè¿™ä¸ªçŸ©é˜µçš„å…ƒç´ æ˜¯äºŒè¿›åˆ¶çš„å‘¢ï¼Œè¦å¥½å¥½åˆ©ç”¨è¿™ä¸ªç‰¹ç‚¹
    H=(X@Theta.T)*R
    cost=0.5*np.sum((H-Y)**2)
    reg=(l/2.)*(np.sum(X**2)+np.sum(Theta**2))

    return cost+reg


def gradient(XTvector,Y,R,nm=1682,nu=943,nf=10,l=0):
    X,Theta=params_seperate(XTvector,nm,nu,nf)

    error=(X@Theta.T)*R -Y #(1682,943)
    Xgrad=error@Theta+l*X      #(1682,10)
    Thetagrad=error.T@X+l*Theta   #(943,10)

    gradvector=np.r_[Xgrad.ravel(),Thetagrad.ravel()]
    return gradvector      #(26250,)


Xsub,Thetasub,Ysub,Rsub=data(X,Theta,Y,R,5,4,3)
XTvectorsub=params_vector(Xsub,Thetasub)
costsub=Cost(XTvectorsub,Ysub,Rsub,5,4,3,1.5)  
gradvectorsub=gradient(XTvectorsub,Ysub,Rsub,5,4,3,1.5)

print (costsub,'\n',gradvectorsub)
#learningrate=0æ—¶costä¸º22.224603725685675;learningrate=1æ—¶costä¸º31.344056244274213


def checkGradient(XTvector,Y,R,nm=1682,nu=943,nf=10,l=0):
    gradvector=gradient(XTvector,Y,R,nm,nu,nf,l)

    l=len(XTvector)
    evec=np.zeros(l)

    idx=np.random.choice(l)
    evec[idx]=0.0001

    loss1=Cost(XTvector+evec,Y,R,nm,nu,nf,l)
    loss2=Cost(XTvector-evec,Y,R,nm,nu,nf,l)

    numgrad=(loss1-loss2)/(2*0.0001)
    difference=(numgrad-gradvector[idx])/(numgrad+gradvector[idx])

    return difference


print (checkGradient(XTvectorsub,Ysub,Rsub,5,4,3,1.5)) #éšæœºå€¼-1.4292438337096058e-09 \ 8.379931319177953e-09 \ ...

