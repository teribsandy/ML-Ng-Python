# coding: utf-8
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

path ='/Users/zhangying/Desktop/243-ML-Ng/ex1-linear regression/ex1data2.txt'

data = pd.read_csv(path, header=None, names=['Size', 'Bedrooms','Price']) #dataçš„shapeä¸º(47,3) ç±»å‹ä¸ºDataFrame

x=data['Size'] #xçš„shapeä¸º(47,) xçš„ç±»å‹ä¸ºSeries
y=data['Bedrooms']
z=data['Price']
ax=plt.subplot(211,projection='3d')
ax.scatter3D(x,y,z,cmap='rainbow') #scatter3Dä¸­xyzæ•°æ®çš„ç±»å‹ä¸ºarrayæˆ–series
ax.set_xlabel('Size'),ax.set_ylabel('Bedrooms'),ax.set_zlabel('Price'),ax.set_title('MLR_Scatter')


# ç‰¹å¾å½’ä¸€åŒ–è¶…çº§é‡è¦ï¼ç”±äºæˆ‘ä¹‹å‰æ²¡æœ‰ç‰¹å¾å½’ä¸€åŒ–ï¼Œå¤šå¤„æŠ¥é”™ï¼
#.apply() ä½œç”¨äºdataframeä¸Šï¼Œç”¨äºå¯¹rowæˆ–è€…columnè¿›è¡Œè®¡ç®—(æ ¹æ®axisæŒ‡å®šï¼Œé»˜è®¤axis=0)
#.applymap()ä½œç”¨äºdataframeä¸Šçš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œæ˜¯å…ƒç´ çº§åˆ«çš„æ“ä½œ
# #.map()ä½œç”¨äºseriesä¸Šï¼Œæ˜¯å…ƒç´ çº§åˆ«çš„æ“ä½œ
def normalize_feature(df):
    return df.apply(lambda column : (column-column.mean())/column.std())
data=normalize_feature(data)    
#æˆ–è€…é‡‡ç”¨ğŸ‘‡ä¹Ÿå¯ä»¥ç‰¹å¾å½’ä¸€åŒ–
#data=(data-data.mean())/data.std() #ç‰¹å¾å½’ä¸€åŒ–

data.insert(0,'Ones',1) #dataçš„shapeä¸º(47,4)
cols=data.shape[1] #data.shape (47,4), cols=4,åˆ—æ•°
X=data.iloc[:,0:cols-1]  #Xæ˜¯dataframeï¼Œä»£è¡¨å‰3åˆ—çš„æ•°æ®ï¼Œ(47,3)
y=data.iloc[:,cols-1:cols] #yæ˜¯dataframeï¼Œä»£è¡¨Priceåˆ—çš„æ•°æ®ï¼Œ(47,1)

X=np.matrix(X.values) #X.valueæ˜¯arrayï¼ŒXæ˜¯matrix     X.shape(47, 3)
y=np.matrix(y.values)  # y.shape  (47, 1)
theta=np.matrix([0,0,0]) #thetaè¾“å‡ºä¸ºmatrix([[0,0,0]])    theta.shape(1, 3)


#computeCostå‡½æ•°ä¸­Xï¼Œyï¼Œthetaéƒ½æ˜¯numpyçŸ©é˜µ
def computeCost(X,y,theta):
    #inner=np.power(((X*theta.T)-y),2)
    inner=np.array((X*theta.T)-y)**2 #å¦‚æœç”¨**2çš„æ–¹å¼è¡¨ç¤ºaä¸­çš„å…ƒç´ å¹³æ–¹ï¼Œé‚£ä¹ˆaå¿…é¡»æ˜¯square array
    return np.sum(inner)/(2*len(X))

#print (computeCost(X,y,theta)) # ç‰¹å¾å½’ä¸€åŒ–å‰çš„åˆå§‹æˆæœ¬65591548106  ç‰¹å¾å½’ä¸€åŒ–åçš„åˆå§‹æˆæœ¬0.489361702128


def gradientDescent(X,y,theta,alpha,epochs):
    temp=np.matrix(np.ones(theta.shape))
    cost=np.zeros(epochs)

    for i in range(epochs):
        error=X*theta.T -y

        #ç”¨for å¾ªç¯   Or    ç›´æ¥åˆ©ç”¨å‘é‡åŒ–ä¸€æ­¥æ±‚è§£
        temp=theta-(alpha/len(X))* error.T * X
        # for j in range(0,X.shape[1]): #j=0,1,2
        #     Xj=X[:,j]
        #     # theta[0,j]-=(alpha/len(X))*np.sum(np.multiply(error,Xj)) #wrong,å½“æ›´æ–°theta1æ—¶ï¼Œerrorä¸­çš„thetaçš„theta0å·²ç»è¢«æ›´æ–°è¿‡äº†
        #     temp[0,j]=theta[0,j]-(alpha/len(X)) * np.sum( np.multiply(error,Xj) ) 

        theta=temp
        cost[i]=computeCost(X,y,theta)

    return theta, cost

alpha=0.01
epochs=1000

finaltheta,cost=gradientDescent(X,y,theta,alpha,epochs) 


#print (finaltheta, computeCost(X,y,finaltheta))
#[[-1.10995657e-16  8.78503652e-01 -4.69166570e-02]] 
#0.130703369608

# fig2,ax2=plt.subplots(figsize=(12,8))
# ax2.plot(np.arange(epochs),cost,'r')
# ax2.set_xlabel('Iterations')
# ax2.set_ylabel('Cost')
# ax2.set_title('Error vs. Training Epochs')
# plt.show()

ftheta0=np.array(finaltheta)[0,0]
ftheta1=np.array(finaltheta)[0,1]
ftheta2=np.array(finaltheta)[0,2]

x1 = np.linspace(data.Size.min(), data.Size.max(), 100)
x2 = np.linspace(data.Bedrooms.min(), data.Bedrooms.max(), 100)
f = ftheta0 + (ftheta1 * x1) + (ftheta2 * x2)

ax3=plt.subplot(212,projection='3d')
ax3.plot(x1,x2,f,'r',label='Hypothesis')
ax3.scatter(data.Size,data.Bedrooms,data['Price'],label='TrainingSet')
ax3.legend(loc=1)
ax3.set_xlabel('Size'),ax3.set_ylabel('Bedrooms'),ax3.set_zlabel('Price'),ax3.set_title('predicted price vs. size&bedrooms')
plt.show()





